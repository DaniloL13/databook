# Calidad de los Datos

Uno de los principios más antiguos de la informática nos dice "entra basura, sale basura" y se refiere al hecho de que si un programa recibe como entrada datos incorrectos o sin sentido en la salida tendremos lo mismo. Este principio también se aplica a la ciencia de datos pues el éxito del proceso depende de la validez de los datos que utilizamos. El asegurar la calidad de los datos que recolectamos es uno de los mayores retos de la minería de datos, por esto es muy importante realizar un proceso previo de verificación y *limpieza de datos*. Para darnos una idea de la complejidad de estas tareas, vamos a considerar la recolección de datos para el desarrollo de un [Observatorio de Lesiones](http://conapra.salud.gob.mx/Interior/Observatorio_Nacional_Lesiones.html). El objetivo de estos sistemas según la secretaría de salud es "... generar datos y evidencia científica para la prevención de lesiones ocasionadas por accidentes viales”. Es indispensable para este tipo de sistemas contar con información confiable sobre los accidentes de tráfico, pero como veremos esto no es nada fácil. Para empezar, atender un accidente involucra varias organizaciones, cruz roja, paramédicos, policía, hospitales, call-centers de emergencia, rescatistas, agencias de seguros entre otras. Por otro lado, se involucran transeúntes, vehículos, pasajeros, edificios, reporteros etc. Para obtener información sobre lo sucedido normalmente se debe entrevistar a los testigos e involucrados. Desde aquí empieza el problema, ya que cada persona tiene su versión de los hechos, puede cambiarla o estar incapacitadas para darla. Cada dependencia tiene la urgencia de recolectar solo la información necesaria para cumplir con su propósito y cada una tiene una versión parcial de los hechos. El accidente puede relacionarse con otros eventos posteriores, por ejemplo una persona puede sufrir secuelas del accidente mucho tiempo después. Entonces, la tarea de integrar los datos requiere la cooperación entre distintas dependencias lo cual nos da un primer golpe de realidad: **En muchas ocasiones no esta en nuestras manos la calidad de los datos, ni tendremos control sobre la fuente de los mismos**. Por lo anterior la minería de datos se enfoca solamente en dos tareas básicas [ref Tan, Kumar]: (1) la detección y corrección de problemas en los datos y (2) la utilización de algoritmos que puedan tolerar datos de mala calidad. Dicho de otra manera, no es nuestro objetivo ni la gestión ni el control de la calidad de los datos, ya que esto involucraría intervenir en los mismos sistemas de recolección.

Decimos que un producto o servicio es de buena calidad si satisface nuestras necesidades. En el caso de los datos, debemos asegurarnos de que sirvan para  nuestros propósitos. Por ejemplo, una tarea de nuestro observatorio podría ser el optimizar la ubicación de estaciones de rescate. Entre los datos requeridos para este objetivo se debe incluir la ubicación exacta de los accidentes. Aquellos registros que solo incluyan la zona o la calle del accidente no tendrían calidad para esta tarea, pero podrían ser de utilidad o tener buena calidad para otros propósitos; a continuación revisaremos algunos problemas que afectan la calidad de los datos.

### Fuentes de datos poco confiables

Muchas de las colecciones de datos disponibles, no se recolectaron originalmente para realizar una investigación científica, por lo que varían mucho en su autenticidad y calidad. Jianzheng Liu et al.(2015) identificaron  tres diferencias entre la recolección de datos realizada por una empresa comercial y una institución de investigación científica:

1. Las empresas no adoptan procedimientos científicos de recolección de datos, como muestreos aleatorios o procedimientos para evitar el sesgo. Las empresas recolectan información con el objetivo de producir ganancias no para hacer ciencia. Normalmente el muestreo de datos se realiza sobre la población objetivo de las empresas y no sobre el público en general.

2. Los métodos de recolección de datos no son públicos y pueden cambiar sin aviso alguno.

3. Las plataformas comerciales no se pueden hacer responsables de la autenticidad ni la validez de los datos que colectan. Por ejemplo, al utilizar datos de una red social como Twitter debemos considerar a las cuentas de *Bots*, usuarios pagados para producir contenido, anuncios y publicaciones tipo *Spam*.

Como podemos ver, la calidad de los datos no solo se refiere a la medición o la captura en sí, también debemos tener en cuenta el procedimiento utilizado en la recolección.

### Errores de medición y recolección de datos

Llamamos errores de medición a aquellos problemas que suceden en el proceso de medición o captura. En el caso de los atributos continuos, llamaremos error a la diferencia numérica entre el valor real y el medido. Un error de medición se puede dar también cuando un sensor no tiene la precisión adecuada e incluso cuando la medición se trunca al almacenarse en una variable que no tiene la escala o precisión adecuada.

Un error de recolección de datos se refiere a problemas de captura al nivel de objetos o atributos. Por ejemplo, cuando omitimos el valor de un atributo o capturamos un objeto en una categoría equivocada.  

### Valores desconocidos
En muchas ocasiones el valor de algunos de los atributos no se conoce. Por ejemplo, en el caso de un accidente pudo no haberse recolectado la la edad del conductor, el número de pasajeros o la información meteorológica. Normalmente se indica la ausencia del dato utilizando alguna palabra reservada o etiqueta  como NULL, None o Nil. 

## Valores atípicos

### Inconsistencias


### Valores Duplicados

### Datos con Ruido
En algunas publicaciones llaman *ruido* a las imperfecciones en los datos. Tal como sucede cuando hay interferencia en la señal de radio y escuchamos la música con ruido. El ruido en los datos son precisamente los errores de medición, recolección, valores atípicos e inconsistencias en los datos. En ocasiones es necesario perturbar o agregar ruido a los datos ya sea para conservar la privacidad de la fuente o para probar que tan robusto es un algoritmo. Para agregar ruido a un conjunto de datos normalmente se agregan componentes aleatorios. Por ejemplo, se pueden agregar nuevos objetos con atributos aleatorios


Evento-Transito
  ciudad
  latitud,
  longitud,
  lugar
  tipo_incidente,
  limite_velocidad
  condicion_vial
  fecha_hora
  condicion_luz
  hra_llamada = models.TimeField((u"Hora de llamada: "), blank=True)
  hra_escena = models.TimeField((u"Hora en la escena:"), blank=True)
  conductor_alcoholizado = models.IntegerField()
  num_lesionados = models.IntegerField()


Evento-Ambulancia    
edad = models.IntegerField(null=True, blank=True)
sexo = models.IntegerField(choices=sexo_choices, null=True, blank=True)
rol = models.ForeignKey(rol_persona)
gravedad_lesion = models.ForeignKey(gravedad_lesion)
num_ambulancia = models.CharField(max_length=2, null=True, blank=True)
operador = models.CharField(max_length=50, null=True, blank=True)
fecha_hora  
lugar = models.ForeignKey(evento)
vehiculo = models.ForeignKey(vehiculo, null=True, blank=True)
muerte_escena = models.ForeignKey(muerte_escena)
prioridad = models.ForeignKey(prioridad)

Evento-Vehiculo
num_ocupantes = models.IntegerField()
modelo = models.ForeignKey(modelo)
marca = models.ForeignKey(marca)
año = models.IntegerField()
vin = models.IntegerField()
num_muertos = models.IntegerField()
conductor_alcoholizado = models.BooleanField()
num_lesionados = models.PositiveIntegerField()
